# üöÄ Integra√ß√£o com Groq (Substituindo OpenAI)

## üìå Por que Groq?

### Vantagens sobre OpenAI:
- **‚ö° 18x mais r√°pido** - Infer√™ncia ultra-r√°pida (at√© 800 tokens/segundo)
- **üí∞ Mais econ√¥mico** - Pre√ßos mais baixos que OpenAI
- **üéØ Modelos especializados** - LLaMA 3.3 otimizado para conversa√ß√£o
- **üÜì Tier gratuito generoso** - Ideal para desenvolvimento e MVPs
- **üîí Open source friendly** - Usa modelos open source (LLaMA, Mixtral)

### Compara√ß√£o de Custos:

| Modelo | Provedor | Input (1M tokens) | Output (1M tokens) | Velocidade |
|--------|----------|-------------------|-------------------|------------|
| GPT-4 | OpenAI | $30.00 | $60.00 | ~50 tokens/s |
| GPT-3.5 Turbo | OpenAI | $0.50 | $1.50 | ~100 tokens/s |
| **LLaMA 3.3 70B** | **Groq** | **$0.59** | **$0.79** | **~800 tokens/s** |
| Mixtral 8x7B | Groq | $0.24 | $0.24 | ~700 tokens/s |
| Gemma 2 9B | Groq | $0.20 | $0.20 | ~850 tokens/s |

---

## ü§ñ Modelos Dispon√≠veis no Groq

### 1. **llama-3.3-70b-versatile** (Recomendado) ‚≠ê
- **Uso**: Conversa√ß√£o, racioc√≠nio complexo, atendimento ao cliente
- **Pontos fortes**: Melhor custo-benef√≠cio, entende portugu√™s muito bem
- **Velocidade**: ~800 tokens/segundo
- **Contexto**: 128k tokens
- **Ideal para**: Nosso chatbot de vendas, classifica√ß√£o de inten√ß√µes

### 2. **llama-3.1-70b-versatile**
- **Uso**: Prop√≥sito geral, racioc√≠nio
- **Pontos fortes**: Vers√£o anterior est√°vel do 3.3
- **Velocidade**: ~750 tokens/segundo
- **Contexto**: 128k tokens

### 3. **mixtral-8x7b-32768**
- **Uso**: Tarefas r√°pidas, classifica√ß√£o
- **Pontos fortes**: Mais barato, muito r√°pido
- **Velocidade**: ~700 tokens/segundo
- **Contexto**: 32k tokens
- **Ideal para**: Extra√ß√£o de inten√ß√£o r√°pida

### 4. **gemma-2-9b-it**
- **Uso**: Tarefas simples, chatbot b√°sico
- **Pontos fortes**: Menor custo, extremamente r√°pido
- **Velocidade**: ~850 tokens/segundo
- **Contexto**: 8k tokens
- **Ideal para**: Respostas r√°pidas e curtas

---

## üõ†Ô∏è Implementa√ß√£o

### Arquivos Criados/Modificados:

1. **`src/lib/groq.ts`** - Nova biblioteca de integra√ß√£o com Groq
   - `chatCompletion()` - Fun√ß√£o gen√©rica de chat
   - `salesChatCompletion()` - Chat otimizado para vendas
   - `extractIntent()` - Extra√ß√£o de inten√ß√£o do usu√°rio
   - `generateRecommendationReasoning()` - Gera√ß√£o de explica√ß√µes

2. **`src/config/env.ts`** - Adicionado `GROQ_API_KEY`

3. **`src/agents/orchestrator.agent.ts`** - Migrado para Groq

4. **`src/agents/recommendation.agent.ts`** - Migrado para Groq

### Arquivos Mantidos:
- `src/lib/openai.ts` - Mantido para compatibilidade futura

---

## üîë Como Obter a API Key

1. Acesse: https://console.groq.com/
2. Crie uma conta gratuita
3. V√° em **API Keys** ‚Üí **Create API Key**
4. Copie a chave (formato: `gsk-...`)
5. Adicione no `.env`:

```bash
GROQ_API_KEY="gsk-sua-chave-aqui"
```

### Tier Gratuito Groq:
- **30 requests/min** para LLaMA 3.3 70B
- **14,400 tokens/min** de input
- **Sem limite de uso mensal**
- Ideal para desenvolvimento e testes

---

## üìä Fun√ß√µes Implementadas

### 1. `chatCompletion(messages, options)`
Fun√ß√£o gen√©rica para chat completion.

```typescript
import { chatCompletion } from './lib/groq';

const response = await chatCompletion([
  { role: 'system', content: 'Voc√™ √© um assistente √∫til.' },
  { role: 'user', content: 'Ol√°!' }
], {
  model: 'llama-3.3-70b-versatile',
  temperature: 0.7,
  maxTokens: 500
});
```

### 2. `salesChatCompletion(userMessage, context)`
Chat otimizado para vendas com prompt pr√©-configurado.

```typescript
import { salesChatCompletion } from './lib/groq';

const response = await salesChatCompletion(
  'Quero um carro para fam√≠lia',
  'Cliente j√° respondeu quiz, or√ßamento 50k'
);
```

### 3. `extractIntent(userMessage)`
Extrai inten√ß√£o do usu√°rio (QUALIFICAR, HUMANO, DUVIDA, OUTRO).

```typescript
import { extractIntent } from './lib/groq';

const intent = await extractIntent('Quero comprar um carro');
// Retorna: 'QUALIFICAR'
```

### 4. `generateRecommendationReasoning(vehicleInfo, userProfile, matchScore)`
Gera explica√ß√£o personalizada para recomenda√ß√£o de ve√≠culo.

```typescript
import { generateRecommendationReasoning } from './lib/groq';

const reasoning = await generateRecommendationReasoning(
  'Onix LT 2020, 42.000 km, R$ 48.000',
  'Or√ßamento R$ 50.000, uso cidade, 5 pessoas',
  95
);
```

---

## üß™ Modo MOCK (Desenvolvimento sem API Key)

O sistema continua funcionando sem API key para testes locais:

```bash
# .env
GROQ_API_KEY="gsk-mock-key-for-development"
```

Quando em modo MOCK:
- Retorna respostas pr√©-definidas
- N√£o consome API calls
- Ideal para testes de fluxo

---

## üéØ Prompt Engineering para Groq

### Boas Pr√°ticas:

1. **Seja espec√≠fico e direto**
   ```typescript
   // ‚ùå Ruim
   "Fale sobre carros"
   
   // ‚úÖ Bom
   "Voc√™ √© um vendedor de carros usados. Explique em 1 frase por que este Onix 2020 √© bom para cidade."
   ```

2. **Use system prompts estruturados**
   ```typescript
   const systemPrompt = `Voc√™ √© um assistente de vendas.
   
   REGRAS:
   - Seja breve (m√°x 3 linhas)
   - Use tom profissional
   - N√£o invente informa√ß√µes`;
   ```

3. **Temperature apropriada**
   - `0.3` - Tarefas de classifica√ß√£o (intent)
   - `0.7` - Conversa√ß√£o natural
   - `0.9` - Criatividade (descri√ß√µes de produtos)

4. **Limite de tokens**
   - Intent: 10 tokens
   - Reasoning: 50 tokens
   - Chat: 300 tokens

---

## üìà Performance Esperada

### Antes (OpenAI GPT-4):
- Lat√™ncia: ~2-3 segundos por resposta
- Custo: $0.03 por 1k tokens (input)
- Timeout: Comum em hor√°rios de pico

### Depois (Groq LLaMA 3.3 70B):
- Lat√™ncia: ~200-400ms por resposta ‚ö°
- Custo: $0.00059 por 1k tokens (input) üí∞
- Timeout: Raro (infraestrutura LPU dedicada)

### Melhoria:
- **7-15x mais r√°pido**
- **50x mais barato**
- **Experi√™ncia do usu√°rio muito melhor**

---

## üîÑ Migra√ß√£o de OpenAI ‚Üí Groq

### Compatibilidade:
O c√≥digo √© quase id√™ntico, pois Groq usa o mesmo formato de API do OpenAI.

```typescript
// Antes (OpenAI)
import OpenAI from 'openai';
const openai = new OpenAI({ apiKey: 'sk-...' });

// Depois (Groq)
import Groq from 'groq-sdk';
const groq = new Groq({ apiKey: 'gsk-...' });
```

### Diferen√ßas principais:
1. **Models**: `gpt-4` ‚Üí `llama-3.3-70b-versatile`
2. **API Key**: `sk-...` ‚Üí `gsk-...`
3. **Endpoint**: Autom√°tico via SDK

---

## üöÄ Deploy no Railway

O Railway j√° suporta Groq nativamente. Basta adicionar a env var:

```bash
# Railway Environment Variables
GROQ_API_KEY=gsk-sua-chave-aqui
```

---

## üìö Recursos

- **Documenta√ß√£o oficial**: https://console.groq.com/docs
- **Playground**: https://groq.com/
- **Modelos dispon√≠veis**: https://console.groq.com/docs/models
- **Rate limits**: https://console.groq.com/docs/rate-limits
- **Pricing**: https://wow.groq.com/pricing/

---

## ‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes

### Limita√ß√µes do Tier Gratuito:
- 30 requests/min (LLaMA 3.3 70B)
- Sem garantia de SLA
- Pode ter throttling em picos

### Quando usar OpenAI ainda:
- Produ√ß√£o em escala (tier pago Groq)
- Necessita GPT-4 Vision (Groq n√£o tem)
- Funcionalidades espec√≠ficas do OpenAI (function calling avan√ßado)

### Recomenda√ß√£o:
- **Desenvolvimento**: Groq (r√°pido e barato)
- **MVP/Beta**: Groq (tier gratuito suficiente)
- **Produ√ß√£o**: Groq tier pago ou OpenAI (depende da escala)

---

## ‚úÖ Checklist de Migra√ß√£o

- [x] Instalar `groq-sdk`
- [x] Criar `src/lib/groq.ts`
- [x] Adicionar `GROQ_API_KEY` no `.env`
- [x] Migrar `OrchestratorAgent`
- [x] Migrar `RecommendationAgent`
- [x] Manter modo MOCK funcionando
- [x] Documentar mudan√ßas
- [ ] Obter API key real do Groq
- [ ] Testar com API key real
- [ ] Fazer commit das mudan√ßas
- [ ] Deploy no Railway

---

**Pronto para usar Groq! üöÄ**

Execute o teste com:
```bash
npm run test:bot
```
